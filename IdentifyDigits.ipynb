{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from resnets_utils import *\n",
    "from keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Get images and convert into Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train = pd.read_csv('train.csv')\n",
    "    train_image = []\n",
    "    for i in tqdm(range(train.shape[0])):\n",
    "        img = image.load_img('train/'+train['filename'][i], target_size=(28,28,1), grayscale=True)\n",
    "        img = image.img_to_array(img)\n",
    "        img = img/255\n",
    "        train_image.append(img)\n",
    "    X = np.array(train_image)\n",
    "    y=train['label'].values\n",
    "    y = to_categorical(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 3\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value. You'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "     \n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X =  Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "      \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = [0.19716813 0.         1.3561227  2.1713073  0.         1.3324987 ]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\", [3, 4, 4, 6])\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    A = identity_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = 'a')\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A], feed_dict={A_prev: X, K.learning_phase(): 0})\n",
    "    print(\"out = \" + str(out[0][1][1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(F2, (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(F3, (1, 1), strides = (1,1), padding = 'valid',name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    ##### SHORTCUT PATH #### (≈2 lines)\n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "   \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = [0.09018463 1.2348977  0.46822017 0.0367176  0.         0.65516603]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\", [3, 4, 4, 6])\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    A = convolutional_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = 'a')\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A], feed_dict={A_prev: X, K.learning_phase(): 0})\n",
    "    print(\"out = \" + str(out[0][1][1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ResNet50(input_shape = (64, 64, 3), classes = 6):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 ->  TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    # Stage 3 (≈4 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    # Stage 4 (≈6 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    # Stage 5 (≈3 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
    "    #X = AveragePooling2D((2, 2), name = 'avg_pool')(X)\n",
    "    \n",
    " \n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to build the model's graph. If your implementation is not correct you will know it by checking your accuracy when running `model.fit(...)` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(input_shape = (28,28, 1), classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49000 [00:00<?, ?it/s]C:\\Users\\nmadapati\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n",
      "100%|██████████| 49000/49000 [00:34<00:00, 1407.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 39200\n",
      "number of test examples = 9800\n",
      "X_train shape: (39200, 28, 28, 1)\n",
      "Y_train shape: (39200, 10)\n",
      "X_test shape: (9800, 28, 28, 1)\n",
      "Y_test shape: (9800, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = load_data()\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6400/39200 [===>..........................] - ETA: 1:06:57 - loss: 0.0408 - acc: 1.00 - ETA: 1:04:58 - loss: 0.0636 - acc: 0.98 - ETA: 1:03:16 - loss: 0.1119 - acc: 0.96 - ETA: 1:02:32 - loss: 0.1480 - acc: 0.95 - ETA: 1:03:43 - loss: 0.2170 - acc: 0.93 - ETA: 1:04:03 - loss: 0.1907 - acc: 0.94 - ETA: 1:04:11 - loss: 0.1869 - acc: 0.95 - ETA: 1:03:57 - loss: 0.1951 - acc: 0.94 - ETA: 1:03:26 - loss: 0.1943 - acc: 0.94 - ETA: 1:03:23 - loss: 0.2005 - acc: 0.93 - ETA: 1:03:17 - loss: 0.1972 - acc: 0.93 - ETA: 1:02:55 - loss: 0.2035 - acc: 0.93 - ETA: 1:02:34 - loss: 0.2011 - acc: 0.93 - ETA: 1:02:13 - loss: 0.1970 - acc: 0.93 - ETA: 1:01:57 - loss: 0.1913 - acc: 0.93 - ETA: 1:02:09 - loss: 0.2051 - acc: 0.92 - ETA: 1:02:16 - loss: 0.1973 - acc: 0.92 - ETA: 1:02:24 - loss: 0.2030 - acc: 0.92 - ETA: 1:02:45 - loss: 0.2071 - acc: 0.93 - ETA: 1:02:55 - loss: 0.2021 - acc: 0.93 - ETA: 1:03:25 - loss: 0.2031 - acc: 0.93 - ETA: 1:03:22 - loss: 0.1994 - acc: 0.93 - ETA: 1:03:27 - loss: 0.2381 - acc: 0.93 - ETA: 1:03:37 - loss: 0.2402 - acc: 0.93 - ETA: 1:03:39 - loss: 0.2562 - acc: 0.92 - ETA: 1:03:30 - loss: 0.2538 - acc: 0.92 - ETA: 1:03:20 - loss: 0.2573 - acc: 0.92 - ETA: 1:03:10 - loss: 0.2537 - acc: 0.92 - ETA: 1:02:59 - loss: 0.2534 - acc: 0.92 - ETA: 1:02:51 - loss: 0.2707 - acc: 0.92 - ETA: 1:02:48 - loss: 0.2668 - acc: 0.92 - ETA: 1:02:39 - loss: 0.2690 - acc: 0.92 - ETA: 1:02:30 - loss: 0.2693 - acc: 0.92 - ETA: 1:02:26 - loss: 0.2700 - acc: 0.92 - ETA: 1:02:32 - loss: 0.2638 - acc: 0.92 - ETA: 1:02:31 - loss: 0.2617 - acc: 0.92 - ETA: 1:02:22 - loss: 0.2617 - acc: 0.92 - ETA: 1:02:20 - loss: 0.2619 - acc: 0.92 - ETA: 1:02:17 - loss: 0.2587 - acc: 0.92 - ETA: 1:02:10 - loss: 0.2587 - acc: 0.92 - ETA: 1:02:10 - loss: 0.2574 - acc: 0.92 - ETA: 1:02:03 - loss: 0.2594 - acc: 0.92 - ETA: 1:01:57 - loss: 0.2702 - acc: 0.92 - ETA: 1:01:50 - loss: 0.2662 - acc: 0.92 - ETA: 1:01:50 - loss: 0.2606 - acc: 0.92 - ETA: 1:01:56 - loss: 0.2580 - acc: 0.92 - ETA: 1:01:48 - loss: 0.2546 - acc: 0.92 - ETA: 1:01:42 - loss: 0.2505 - acc: 0.92 - ETA: 1:01:35 - loss: 0.2543 - acc: 0.92 - ETA: 1:01:29 - loss: 0.2514 - acc: 0.93 - ETA: 1:01:27 - loss: 0.2493 - acc: 0.93 - ETA: 1:01:21 - loss: 0.2484 - acc: 0.93 - ETA: 1:01:18 - loss: 0.2455 - acc: 0.93 - ETA: 1:01:12 - loss: 0.2436 - acc: 0.93 - ETA: 1:01:06 - loss: 0.2431 - acc: 0.93 - ETA: 1:01:03 - loss: 0.2412 - acc: 0.93 - ETA: 1:00:58 - loss: 0.2412 - acc: 0.93 - ETA: 1:00:58 - loss: 0.2447 - acc: 0.92 - ETA: 1:00:57 - loss: 0.2427 - acc: 0.92 - ETA: 1:00:52 - loss: 0.2392 - acc: 0.93 - ETA: 1:00:49 - loss: 0.2356 - acc: 0.93 - ETA: 1:00:45 - loss: 0.2410 - acc: 0.93 - ETA: 1:00:40 - loss: 0.2442 - acc: 0.93 - ETA: 1:00:34 - loss: 0.2434 - acc: 0.93 - ETA: 1:00:30 - loss: 0.2409 - acc: 0.93 - ETA: 1:00:25 - loss: 0.2406 - acc: 0.93 - ETA: 1:00:23 - loss: 0.2386 - acc: 0.93 - ETA: 1:00:17 - loss: 0.2386 - acc: 0.93 - ETA: 1:00:12 - loss: 0.2394 - acc: 0.93 - ETA: 1:00:07 - loss: 0.2468 - acc: 0.92 - ETA: 1:00:02 - loss: 0.2452 - acc: 0.93 - ETA: 1:00:00 - loss: 0.2437 - acc: 0.93 - ETA: 59:54 - loss: 0.2413 - acc: 0.9311 - ETA: 59:49 - loss: 0.2450 - acc: 0.92 - ETA: 59:44 - loss: 0.2445 - acc: 0.92 - ETA: 59:39 - loss: 0.2467 - acc: 0.92 - ETA: 59:38 - loss: 0.2470 - acc: 0.92 - ETA: 59:35 - loss: 0.2444 - acc: 0.92 - ETA: 59:30 - loss: 0.2425 - acc: 0.93 - ETA: 59:32 - loss: 0.2457 - acc: 0.93 - ETA: 59:31 - loss: 0.2430 - acc: 0.93 - ETA: 59:32 - loss: 0.2411 - acc: 0.93 - ETA: 59:33 - loss: 0.2421 - acc: 0.93 - ETA: 59:31 - loss: 0.2427 - acc: 0.93 - ETA: 59:28 - loss: 0.2456 - acc: 0.93 - ETA: 59:27 - loss: 0.2433 - acc: 0.93 - ETA: 59:29 - loss: 0.2419 - acc: 0.93 - ETA: 59:24 - loss: 0.2418 - acc: 0.93 - ETA: 59:23 - loss: 0.2429 - acc: 0.93 - ETA: 59:22 - loss: 0.2408 - acc: 0.93 - ETA: 59:18 - loss: 0.2402 - acc: 0.93 - ETA: 59:16 - loss: 0.2417 - acc: 0.93 - ETA: 59:11 - loss: 0.2397 - acc: 0.93 - ETA: 59:07 - loss: 0.2499 - acc: 0.92 - ETA: 59:02 - loss: 0.2491 - acc: 0.92 - ETA: 58:58 - loss: 0.2477 - acc: 0.92 - ETA: 59:00 - loss: 0.2463 - acc: 0.93 - ETA: 58:56 - loss: 0.2486 - acc: 0.92 - ETA: 58:51 - loss: 0.2498 - acc: 0.92 - ETA: 58:47 - loss: 0.2484 - acc: 0.92 - ETA: 58:42 - loss: 0.2516 - acc: 0.92 - ETA: 58:40 - loss: 0.2510 - acc: 0.92 - ETA: 58:36 - loss: 0.2512 - acc: 0.92 - ETA: 58:31 - loss: 0.2495 - acc: 0.92 - ETA: 58:27 - loss: 0.2526 - acc: 0.92 - ETA: 58:22 - loss: 0.2513 - acc: 0.92 - ETA: 58:18 - loss: 0.2512 - acc: 0.92 - ETA: 58:16 - loss: 0.2505 - acc: 0.92 - ETA: 58:11 - loss: 0.2493 - acc: 0.92 - ETA: 58:07 - loss: 0.2496 - acc: 0.92 - ETA: 58:05 - loss: 0.2490 - acc: 0.92 - ETA: 58:04 - loss: 0.2470 - acc: 0.92 - ETA: 58:02 - loss: 0.2474 - acc: 0.92 - ETA: 57:57 - loss: 0.2456 - acc: 0.92 - ETA: 57:53 - loss: 0.2441 - acc: 0.92 - ETA: 57:50 - loss: 0.2435 - acc: 0.93 - ETA: 57:46 - loss: 0.2428 - acc: 0.92 - ETA: 57:43 - loss: 0.2421 - acc: 0.92 - ETA: 57:39 - loss: 0.2428 - acc: 0.92 - ETA: 57:35 - loss: 0.2417 - acc: 0.92 - ETA: 57:31 - loss: 0.2431 - acc: 0.92 - ETA: 57:30 - loss: 0.2418 - acc: 0.92 - ETA: 57:28 - loss: 0.2401 - acc: 0.92 - ETA: 57:24 - loss: 0.2385 - acc: 0.93 - ETA: 57:19 - loss: 0.2423 - acc: 0.92 - ETA: 57:15 - loss: 0.2435 - acc: 0.92 - ETA: 57:11 - loss: 0.2426 - acc: 0.92 - ETA: 57:08 - loss: 0.2428 - acc: 0.92 - ETA: 57:05 - loss: 0.2432 - acc: 0.92 - ETA: 57:02 - loss: 0.2443 - acc: 0.92 - ETA: 57:01 - loss: 0.2447 - acc: 0.92 - ETA: 56:57 - loss: 0.2434 - acc: 0.92 - ETA: 56:54 - loss: 0.2422 - acc: 0.92 - ETA: 56:50 - loss: 0.2424 - acc: 0.92 - ETA: 56:46 - loss: 0.2411 - acc: 0.92 - ETA: 56:43 - loss: 0.2409 - acc: 0.92 - ETA: 56:39 - loss: 0.2408 - acc: 0.92 - ETA: 56:35 - loss: 0.2406 - acc: 0.92 - ETA: 56:32 - loss: 0.2468 - acc: 0.92 - ETA: 56:28 - loss: 0.2457 - acc: 0.92 - ETA: 56:24 - loss: 0.2462 - acc: 0.92 - ETA: 56:20 - loss: 0.2460 - acc: 0.92 - ETA: 56:16 - loss: 0.2481 - acc: 0.92 - ETA: 56:13 - loss: 0.2487 - acc: 0.92 - ETA: 56:09 - loss: 0.2477 - acc: 0.92 - ETA: 56:05 - loss: 0.2478 - acc: 0.92 - ETA: 56:01 - loss: 0.2468 - acc: 0.92 - ETA: 55:57 - loss: 0.2456 - acc: 0.92 - ETA: 55:55 - loss: 0.2459 - acc: 0.92 - ETA: 55:54 - loss: 0.2456 - acc: 0.92 - ETA: 55:52 - loss: 0.2444 - acc: 0.92 - ETA: 55:49 - loss: 0.2478 - acc: 0.92 - ETA: 55:47 - loss: 0.2465 - acc: 0.92 - ETA: 55:45 - loss: 0.2469 - acc: 0.92 - ETA: 55:42 - loss: 0.2477 - acc: 0.92 - ETA: 55:39 - loss: 0.2478 - acc: 0.92 - ETA: 55:35 - loss: 0.2480 - acc: 0.92 - ETA: 55:31 - loss: 0.2510 - acc: 0.92 - ETA: 55:28 - loss: 0.2504 - acc: 0.92 - ETA: 55:25 - loss: 0.2494 - acc: 0.92 - ETA: 55:21 - loss: 0.2481 - acc: 0.92 - ETA: 55:18 - loss: 0.2482 - acc: 0.92 - ETA: 55:14 - loss: 0.2469 - acc: 0.92 - ETA: 55:11 - loss: 0.2488 - acc: 0.92 - ETA: 55:08 - loss: 0.2545 - acc: 0.92 - ETA: 55:04 - loss: 0.2530 - acc: 0.92 - ETA: 55:00 - loss: 0.2526 - acc: 0.92 - ETA: 54:56 - loss: 0.2513 - acc: 0.92 - ETA: 54:53 - loss: 0.2508 - acc: 0.92 - ETA: 54:50 - loss: 0.2510 - acc: 0.92 - ETA: 54:46 - loss: 0.2509 - acc: 0.92 - ETA: 54:42 - loss: 0.2506 - acc: 0.92 - ETA: 54:39 - loss: 0.2506 - acc: 0.92 - ETA: 54:35 - loss: 0.2540 - acc: 0.92 - ETA: 54:32 - loss: 0.2533 - acc: 0.92 - ETA: 54:28 - loss: 0.2534 - acc: 0.92 - ETA: 54:25 - loss: 0.2530 - acc: 0.92 - ETA: 54:21 - loss: 0.2521 - acc: 0.92 - ETA: 54:18 - loss: 0.2518 - acc: 0.92 - ETA: 54:15 - loss: 0.2506 - acc: 0.92 - ETA: 54:11 - loss: 0.2501 - acc: 0.92 - ETA: 54:08 - loss: 0.2490 - acc: 0.93 - ETA: 54:04 - loss: 0.2486 - acc: 0.93 - ETA: 54:00 - loss: 0.2502 - acc: 0.93 - ETA: 53:58 - loss: 0.2496 - acc: 0.93 - ETA: 53:54 - loss: 0.2489 - acc: 0.93 - ETA: 53:51 - loss: 0.2486 - acc: 0.93 - ETA: 53:47 - loss: 0.2479 - acc: 0.93 - ETA: 53:44 - loss: 0.2469 - acc: 0.93 - ETA: 53:40 - loss: 0.2458 - acc: 0.93 - ETA: 53:38 - loss: 0.2451 - acc: 0.93 - ETA: 53:34 - loss: 0.2444 - acc: 0.93 - ETA: 53:32 - loss: 0.2437 - acc: 0.93 - ETA: 53:32 - loss: 0.2434 - acc: 0.93 - ETA: 53:29 - loss: 0.2434 - acc: 0.93 - ETA: 53:28 - loss: 0.2424 - acc: 0.93 - ETA: 53:24 - loss: 0.2441 - acc: 0.93 - ETA: 53:21 - loss: 0.2441 - acc: 0.93 - ETA: 53:18 - loss: 0.2432 - acc: 0.93 - ETA: 53:15 - loss: 0.2441 - acc: 0.9325\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12928/39200 [========>.....................] - ETA: 53:12 - loss: 0.2440 - acc: 0.93 - ETA: 53:08 - loss: 0.2434 - acc: 0.93 - ETA: 53:05 - loss: 0.2432 - acc: 0.93 - ETA: 53:01 - loss: 0.2430 - acc: 0.93 - ETA: 52:59 - loss: 0.2450 - acc: 0.93 - ETA: 52:56 - loss: 0.2454 - acc: 0.93 - ETA: 52:52 - loss: 0.2449 - acc: 0.93 - ETA: 52:48 - loss: 0.2447 - acc: 0.93 - ETA: 52:46 - loss: 0.2447 - acc: 0.93 - ETA: 52:42 - loss: 0.2444 - acc: 0.93 - ETA: 52:39 - loss: 0.2442 - acc: 0.93 - ETA: 52:35 - loss: 0.2441 - acc: 0.93 - ETA: 52:32 - loss: 0.2445 - acc: 0.93 - ETA: 52:28 - loss: 0.2462 - acc: 0.93 - ETA: 52:24 - loss: 0.2454 - acc: 0.93 - ETA: 52:22 - loss: 0.2452 - acc: 0.93 - ETA: 52:18 - loss: 0.2455 - acc: 0.93 - ETA: 52:14 - loss: 0.2444 - acc: 0.93 - ETA: 52:11 - loss: 0.2450 - acc: 0.93 - ETA: 52:07 - loss: 0.2442 - acc: 0.93 - ETA: 52:04 - loss: 0.2437 - acc: 0.93 - ETA: 52:01 - loss: 0.2462 - acc: 0.93 - ETA: 51:58 - loss: 0.2452 - acc: 0.93 - ETA: 51:55 - loss: 0.2450 - acc: 0.93 - ETA: 51:51 - loss: 0.2443 - acc: 0.93 - ETA: 51:48 - loss: 0.2440 - acc: 0.93 - ETA: 51:45 - loss: 0.2431 - acc: 0.93 - ETA: 51:41 - loss: 0.2453 - acc: 0.93 - ETA: 51:38 - loss: 0.2449 - acc: 0.93 - ETA: 51:34 - loss: 0.2443 - acc: 0.93 - ETA: 51:30 - loss: 0.2449 - acc: 0.93 - ETA: 51:28 - loss: 0.2447 - acc: 0.93 - ETA: 51:25 - loss: 0.2455 - acc: 0.93 - ETA: 51:21 - loss: 0.2454 - acc: 0.93 - ETA: 51:18 - loss: 0.2447 - acc: 0.93 - ETA: 51:14 - loss: 0.2438 - acc: 0.93 - ETA: 51:11 - loss: 0.2430 - acc: 0.93 - ETA: 51:08 - loss: 0.2426 - acc: 0.93 - ETA: 51:04 - loss: 0.2421 - acc: 0.93 - ETA: 51:01 - loss: 0.2421 - acc: 0.93 - ETA: 50:57 - loss: 0.2421 - acc: 0.93 - ETA: 50:54 - loss: 0.2413 - acc: 0.93 - ETA: 50:51 - loss: 0.2408 - acc: 0.93 - ETA: 50:47 - loss: 0.2398 - acc: 0.93 - ETA: 50:44 - loss: 0.2407 - acc: 0.93 - ETA: 50:40 - loss: 0.2405 - acc: 0.93 - ETA: 50:37 - loss: 0.2406 - acc: 0.93 - ETA: 50:34 - loss: 0.2398 - acc: 0.93 - ETA: 50:31 - loss: 0.2400 - acc: 0.93 - ETA: 50:27 - loss: 0.2393 - acc: 0.93 - ETA: 50:23 - loss: 0.2384 - acc: 0.93 - ETA: 50:20 - loss: 0.2390 - acc: 0.93 - ETA: 50:17 - loss: 0.2383 - acc: 0.93 - ETA: 50:14 - loss: 0.2377 - acc: 0.93 - ETA: 50:10 - loss: 0.2389 - acc: 0.93 - ETA: 50:07 - loss: 0.2388 - acc: 0.93 - ETA: 50:03 - loss: 0.2404 - acc: 0.93 - ETA: 50:00 - loss: 0.2407 - acc: 0.93 - ETA: 49:57 - loss: 0.2399 - acc: 0.93 - ETA: 49:53 - loss: 0.2401 - acc: 0.93 - ETA: 49:50 - loss: 0.2396 - acc: 0.93 - ETA: 49:47 - loss: 0.2409 - acc: 0.93 - ETA: 49:44 - loss: 0.2403 - acc: 0.93 - ETA: 49:41 - loss: 0.2399 - acc: 0.93 - ETA: 49:37 - loss: 0.2395 - acc: 0.93 - ETA: 49:34 - loss: 0.2406 - acc: 0.93 - ETA: 49:30 - loss: 0.2406 - acc: 0.93 - ETA: 49:27 - loss: 0.2400 - acc: 0.93 - ETA: 49:24 - loss: 0.2400 - acc: 0.93 - ETA: 49:21 - loss: 0.2398 - acc: 0.93 - ETA: 49:17 - loss: 0.2410 - acc: 0.93 - ETA: 49:14 - loss: 0.2403 - acc: 0.93 - ETA: 49:11 - loss: 0.2395 - acc: 0.93 - ETA: 49:08 - loss: 0.2394 - acc: 0.93 - ETA: 49:04 - loss: 0.2390 - acc: 0.93 - ETA: 49:01 - loss: 0.2385 - acc: 0.93 - ETA: 48:58 - loss: 0.2377 - acc: 0.93 - ETA: 48:54 - loss: 0.2374 - acc: 0.93 - ETA: 48:52 - loss: 0.2374 - acc: 0.93 - ETA: 48:48 - loss: 0.2367 - acc: 0.93 - ETA: 48:45 - loss: 0.2376 - acc: 0.93 - ETA: 48:42 - loss: 0.2370 - acc: 0.93 - ETA: 48:38 - loss: 0.2363 - acc: 0.93 - ETA: 48:36 - loss: 0.2356 - acc: 0.93 - ETA: 48:32 - loss: 0.2377 - acc: 0.93 - ETA: 48:29 - loss: 0.2373 - acc: 0.93 - ETA: 48:26 - loss: 0.2375 - acc: 0.93 - ETA: 48:22 - loss: 0.2368 - acc: 0.93 - ETA: 48:19 - loss: 0.2368 - acc: 0.93 - ETA: 48:16 - loss: 0.2363 - acc: 0.93 - ETA: 48:12 - loss: 0.2360 - acc: 0.93 - ETA: 48:09 - loss: 0.2366 - acc: 0.93 - ETA: 48:06 - loss: 0.2363 - acc: 0.93 - ETA: 48:02 - loss: 0.2361 - acc: 0.93 - ETA: 47:59 - loss: 0.2359 - acc: 0.93 - ETA: 47:56 - loss: 0.2352 - acc: 0.93 - ETA: 47:53 - loss: 0.2347 - acc: 0.93 - ETA: 47:49 - loss: 0.2344 - acc: 0.93 - ETA: 47:46 - loss: 0.2342 - acc: 0.93 - ETA: 47:44 - loss: 0.2348 - acc: 0.93 - ETA: 47:41 - loss: 0.2342 - acc: 0.93 - ETA: 47:38 - loss: 0.2357 - acc: 0.93 - ETA: 47:35 - loss: 0.2367 - acc: 0.93 - ETA: 47:32 - loss: 0.2371 - acc: 0.93 - ETA: 47:29 - loss: 0.2370 - acc: 0.93 - ETA: 47:26 - loss: 0.2374 - acc: 0.93 - ETA: 47:22 - loss: 0.2370 - acc: 0.93 - ETA: 47:19 - loss: 0.2364 - acc: 0.93 - ETA: 47:16 - loss: 0.2365 - acc: 0.93 - ETA: 47:13 - loss: 0.2366 - acc: 0.93 - ETA: 47:10 - loss: 0.2366 - acc: 0.93 - ETA: 47:07 - loss: 0.2361 - acc: 0.93 - ETA: 47:03 - loss: 0.2357 - acc: 0.93 - ETA: 47:00 - loss: 0.2357 - acc: 0.93 - ETA: 46:57 - loss: 0.2368 - acc: 0.93 - ETA: 46:54 - loss: 0.2379 - acc: 0.93 - ETA: 46:50 - loss: 0.2373 - acc: 0.93 - ETA: 46:47 - loss: 0.2387 - acc: 0.93 - ETA: 46:43 - loss: 0.2382 - acc: 0.93 - ETA: 46:40 - loss: 0.2379 - acc: 0.93 - ETA: 46:37 - loss: 0.2376 - acc: 0.93 - ETA: 46:34 - loss: 0.2373 - acc: 0.93 - ETA: 46:31 - loss: 0.2390 - acc: 0.93 - ETA: 46:27 - loss: 0.2386 - acc: 0.93 - ETA: 46:24 - loss: 0.2382 - acc: 0.93 - ETA: 46:21 - loss: 0.2379 - acc: 0.93 - ETA: 46:18 - loss: 0.2381 - acc: 0.93 - ETA: 46:14 - loss: 0.2378 - acc: 0.93 - ETA: 46:11 - loss: 0.2389 - acc: 0.93 - ETA: 46:08 - loss: 0.2383 - acc: 0.93 - ETA: 46:05 - loss: 0.2377 - acc: 0.93 - ETA: 46:02 - loss: 0.2375 - acc: 0.93 - ETA: 45:58 - loss: 0.2373 - acc: 0.93 - ETA: 45:55 - loss: 0.2367 - acc: 0.93 - ETA: 45:52 - loss: 0.2364 - acc: 0.93 - ETA: 45:48 - loss: 0.2361 - acc: 0.93 - ETA: 45:46 - loss: 0.2369 - acc: 0.93 - ETA: 45:42 - loss: 0.2370 - acc: 0.93 - ETA: 45:39 - loss: 0.2401 - acc: 0.93 - ETA: 45:36 - loss: 0.2410 - acc: 0.93 - ETA: 45:32 - loss: 0.2411 - acc: 0.93 - ETA: 45:30 - loss: 0.2412 - acc: 0.93 - ETA: 45:26 - loss: 0.2406 - acc: 0.93 - ETA: 45:23 - loss: 0.2405 - acc: 0.93 - ETA: 45:19 - loss: 0.2398 - acc: 0.93 - ETA: 45:16 - loss: 0.2392 - acc: 0.93 - ETA: 45:13 - loss: 0.2386 - acc: 0.93 - ETA: 45:10 - loss: 0.2383 - acc: 0.93 - ETA: 45:07 - loss: 0.2388 - acc: 0.93 - ETA: 45:04 - loss: 0.2395 - acc: 0.93 - ETA: 45:01 - loss: 0.2389 - acc: 0.93 - ETA: 44:58 - loss: 0.2410 - acc: 0.93 - ETA: 44:55 - loss: 0.2419 - acc: 0.93 - ETA: 44:51 - loss: 0.2414 - acc: 0.93 - ETA: 44:48 - loss: 0.2422 - acc: 0.93 - ETA: 44:45 - loss: 0.2424 - acc: 0.93 - ETA: 44:41 - loss: 0.2420 - acc: 0.93 - ETA: 44:38 - loss: 0.2425 - acc: 0.93 - ETA: 44:35 - loss: 0.2429 - acc: 0.93 - ETA: 44:32 - loss: 0.2425 - acc: 0.93 - ETA: 44:28 - loss: 0.2426 - acc: 0.93 - ETA: 44:25 - loss: 0.2424 - acc: 0.93 - ETA: 44:23 - loss: 0.2460 - acc: 0.93 - ETA: 44:19 - loss: 0.2463 - acc: 0.93 - ETA: 44:16 - loss: 0.2464 - acc: 0.93 - ETA: 44:13 - loss: 0.2459 - acc: 0.93 - ETA: 44:09 - loss: 0.2457 - acc: 0.93 - ETA: 44:06 - loss: 0.2455 - acc: 0.93 - ETA: 44:03 - loss: 0.2449 - acc: 0.93 - ETA: 44:00 - loss: 0.2447 - acc: 0.93 - ETA: 43:57 - loss: 0.2450 - acc: 0.93 - ETA: 43:53 - loss: 0.2446 - acc: 0.93 - ETA: 43:50 - loss: 0.2443 - acc: 0.93 - ETA: 43:47 - loss: 0.2444 - acc: 0.93 - ETA: 43:44 - loss: 0.2443 - acc: 0.93 - ETA: 43:41 - loss: 0.2443 - acc: 0.93 - ETA: 43:38 - loss: 0.2447 - acc: 0.93 - ETA: 43:35 - loss: 0.2445 - acc: 0.93 - ETA: 43:32 - loss: 0.2448 - acc: 0.93 - ETA: 43:28 - loss: 0.2457 - acc: 0.93 - ETA: 43:25 - loss: 0.2454 - acc: 0.93 - ETA: 43:22 - loss: 0.2462 - acc: 0.93 - ETA: 43:19 - loss: 0.2456 - acc: 0.93 - ETA: 43:16 - loss: 0.2454 - acc: 0.93 - ETA: 43:13 - loss: 0.2453 - acc: 0.93 - ETA: 43:09 - loss: 0.2449 - acc: 0.93 - ETA: 43:06 - loss: 0.2445 - acc: 0.93 - ETA: 43:03 - loss: 0.2439 - acc: 0.93 - ETA: 43:00 - loss: 0.2438 - acc: 0.93 - ETA: 42:57 - loss: 0.2437 - acc: 0.93 - ETA: 42:54 - loss: 0.2435 - acc: 0.93 - ETA: 42:50 - loss: 0.2437 - acc: 0.93 - ETA: 42:47 - loss: 0.2434 - acc: 0.93 - ETA: 42:44 - loss: 0.2429 - acc: 0.93 - ETA: 42:41 - loss: 0.2426 - acc: 0.93 - ETA: 42:38 - loss: 0.2423 - acc: 0.93 - ETA: 42:35 - loss: 0.2420 - acc: 0.93 - ETA: 42:32 - loss: 0.2424 - acc: 0.93 - ETA: 42:29 - loss: 0.2421 - acc: 0.93 - ETA: 42:26 - loss: 0.2418 - acc: 0.93 - ETA: 42:22 - loss: 0.2428 - acc: 0.93 - ETA: 42:19 - loss: 0.2428 - acc: 0.93 - ETA: 42:16 - loss: 0.2426 - acc: 0.93 - ETA: 42:13 - loss: 0.2424 - acc: 0.9349"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19456/39200 [=============>................] - ETA: 42:10 - loss: 0.2420 - acc: 0.93 - ETA: 42:07 - loss: 0.2415 - acc: 0.93 - ETA: 42:03 - loss: 0.2426 - acc: 0.93 - ETA: 42:01 - loss: 0.2430 - acc: 0.93 - ETA: 41:58 - loss: 0.2430 - acc: 0.93 - ETA: 41:55 - loss: 0.2426 - acc: 0.93 - ETA: 41:52 - loss: 0.2427 - acc: 0.93 - ETA: 41:49 - loss: 0.2421 - acc: 0.93 - ETA: 41:46 - loss: 0.2418 - acc: 0.93 - ETA: 41:42 - loss: 0.2414 - acc: 0.93 - ETA: 41:39 - loss: 0.2409 - acc: 0.93 - ETA: 41:36 - loss: 0.2420 - acc: 0.93 - ETA: 41:33 - loss: 0.2422 - acc: 0.93 - ETA: 41:30 - loss: 0.2423 - acc: 0.93 - ETA: 41:26 - loss: 0.2424 - acc: 0.93 - ETA: 41:23 - loss: 0.2424 - acc: 0.93 - ETA: 41:20 - loss: 0.2428 - acc: 0.93 - ETA: 41:17 - loss: 0.2431 - acc: 0.93 - ETA: 41:15 - loss: 0.2430 - acc: 0.93 - ETA: 41:12 - loss: 0.2429 - acc: 0.93 - ETA: 41:09 - loss: 0.2427 - acc: 0.93 - ETA: 41:05 - loss: 0.2423 - acc: 0.93 - ETA: 41:02 - loss: 0.2424 - acc: 0.93 - ETA: 40:59 - loss: 0.2419 - acc: 0.93 - ETA: 40:56 - loss: 0.2419 - acc: 0.93 - ETA: 40:54 - loss: 0.2420 - acc: 0.93 - ETA: 40:51 - loss: 0.2418 - acc: 0.93 - ETA: 40:47 - loss: 0.2418 - acc: 0.93 - ETA: 40:44 - loss: 0.2418 - acc: 0.93 - ETA: 40:41 - loss: 0.2415 - acc: 0.93 - ETA: 40:37 - loss: 0.2412 - acc: 0.93 - ETA: 40:35 - loss: 0.2408 - acc: 0.93 - ETA: 40:31 - loss: 0.2403 - acc: 0.93 - ETA: 40:28 - loss: 0.2401 - acc: 0.93 - ETA: 40:25 - loss: 0.2401 - acc: 0.93 - ETA: 40:22 - loss: 0.2396 - acc: 0.93 - ETA: 40:20 - loss: 0.2406 - acc: 0.93 - ETA: 40:17 - loss: 0.2418 - acc: 0.93 - ETA: 40:13 - loss: 0.2415 - acc: 0.93 - ETA: 40:10 - loss: 0.2412 - acc: 0.93 - ETA: 40:07 - loss: 0.2408 - acc: 0.93 - ETA: 40:04 - loss: 0.2415 - acc: 0.93 - ETA: 40:01 - loss: 0.2413 - acc: 0.93 - ETA: 39:58 - loss: 0.2410 - acc: 0.93 - ETA: 39:54 - loss: 0.2406 - acc: 0.93 - ETA: 39:51 - loss: 0.2406 - acc: 0.93 - ETA: 39:48 - loss: 0.2401 - acc: 0.93 - ETA: 39:45 - loss: 0.2403 - acc: 0.93 - ETA: 39:42 - loss: 0.2399 - acc: 0.93 - ETA: 39:38 - loss: 0.2401 - acc: 0.93 - ETA: 39:35 - loss: 0.2398 - acc: 0.93 - ETA: 39:32 - loss: 0.2394 - acc: 0.93 - ETA: 39:29 - loss: 0.2391 - acc: 0.93 - ETA: 39:26 - loss: 0.2386 - acc: 0.93 - ETA: 39:23 - loss: 0.2387 - acc: 0.93 - ETA: 39:19 - loss: 0.2388 - acc: 0.93 - ETA: 39:16 - loss: 0.2389 - acc: 0.93 - ETA: 39:13 - loss: 0.2395 - acc: 0.93 - ETA: 39:10 - loss: 0.2393 - acc: 0.93 - ETA: 39:07 - loss: 0.2398 - acc: 0.93 - ETA: 39:04 - loss: 0.2395 - acc: 0.93 - ETA: 39:01 - loss: 0.2393 - acc: 0.93 - ETA: 38:58 - loss: 0.2389 - acc: 0.93 - ETA: 38:55 - loss: 0.2384 - acc: 0.93 - ETA: 38:52 - loss: 0.2391 - acc: 0.93 - ETA: 38:49 - loss: 0.2388 - acc: 0.93 - ETA: 38:46 - loss: 0.2385 - acc: 0.93 - ETA: 38:43 - loss: 0.2382 - acc: 0.93 - ETA: 38:40 - loss: 0.2378 - acc: 0.93 - ETA: 38:37 - loss: 0.2377 - acc: 0.93 - ETA: 38:34 - loss: 0.2377 - acc: 0.93 - ETA: 38:31 - loss: 0.2388 - acc: 0.93 - ETA: 38:28 - loss: 0.2388 - acc: 0.93 - ETA: 38:25 - loss: 0.2385 - acc: 0.93 - ETA: 38:22 - loss: 0.2382 - acc: 0.93 - ETA: 38:18 - loss: 0.2380 - acc: 0.93 - ETA: 38:15 - loss: 0.2378 - acc: 0.93 - ETA: 38:12 - loss: 0.2376 - acc: 0.93 - ETA: 38:09 - loss: 0.2371 - acc: 0.93 - ETA: 38:06 - loss: 0.2375 - acc: 0.93 - ETA: 38:04 - loss: 0.2371 - acc: 0.93 - ETA: 38:00 - loss: 0.2368 - acc: 0.93 - ETA: 37:57 - loss: 0.2364 - acc: 0.93 - ETA: 37:54 - loss: 0.2366 - acc: 0.93 - ETA: 37:51 - loss: 0.2370 - acc: 0.93 - ETA: 37:48 - loss: 0.2366 - acc: 0.93 - ETA: 37:45 - loss: 0.2363 - acc: 0.93 - ETA: 37:41 - loss: 0.2364 - acc: 0.93 - ETA: 37:38 - loss: 0.2363 - acc: 0.93 - ETA: 37:35 - loss: 0.2359 - acc: 0.93 - ETA: 37:32 - loss: 0.2376 - acc: 0.93 - ETA: 37:29 - loss: 0.2372 - acc: 0.93 - ETA: 37:25 - loss: 0.2373 - acc: 0.93 - ETA: 37:23 - loss: 0.2384 - acc: 0.93 - ETA: 37:19 - loss: 0.2383 - acc: 0.93 - ETA: 37:16 - loss: 0.2381 - acc: 0.93 - ETA: 37:13 - loss: 0.2380 - acc: 0.93 - ETA: 37:10 - loss: 0.2378 - acc: 0.93 - ETA: 37:07 - loss: 0.2380 - acc: 0.93 - ETA: 37:04 - loss: 0.2386 - acc: 0.93 - ETA: 37:01 - loss: 0.2386 - acc: 0.93 - ETA: 36:58 - loss: 0.2388 - acc: 0.93 - ETA: 36:55 - loss: 0.2389 - acc: 0.93 - ETA: 36:52 - loss: 0.2387 - acc: 0.93 - ETA: 36:49 - loss: 0.2382 - acc: 0.93 - ETA: 36:46 - loss: 0.2379 - acc: 0.93 - ETA: 36:43 - loss: 0.2379 - acc: 0.93 - ETA: 36:40 - loss: 0.2379 - acc: 0.93 - ETA: 36:37 - loss: 0.2378 - acc: 0.93 - ETA: 36:34 - loss: 0.2378 - acc: 0.93 - ETA: 36:31 - loss: 0.2374 - acc: 0.93 - ETA: 36:28 - loss: 0.2376 - acc: 0.93 - ETA: 36:25 - loss: 0.2374 - acc: 0.93 - ETA: 36:22 - loss: 0.2373 - acc: 0.93 - ETA: 36:19 - loss: 0.2378 - acc: 0.93 - ETA: 36:15 - loss: 0.2376 - acc: 0.93 - ETA: 36:12 - loss: 0.2373 - acc: 0.93 - ETA: 36:09 - loss: 0.2371 - acc: 0.93 - ETA: 36:06 - loss: 0.2369 - acc: 0.93 - ETA: 36:03 - loss: 0.2370 - acc: 0.93 - ETA: 36:00 - loss: 0.2372 - acc: 0.93 - ETA: 35:57 - loss: 0.2372 - acc: 0.93 - ETA: 35:53 - loss: 0.2392 - acc: 0.93 - ETA: 35:50 - loss: 0.2392 - acc: 0.93 - ETA: 35:47 - loss: 0.2389 - acc: 0.93 - ETA: 35:44 - loss: 0.2405 - acc: 0.93 - ETA: 35:41 - loss: 0.2401 - acc: 0.93 - ETA: 35:38 - loss: 0.2399 - acc: 0.93 - ETA: 35:34 - loss: 0.2402 - acc: 0.93 - ETA: 35:31 - loss: 0.2402 - acc: 0.93 - ETA: 35:28 - loss: 0.2404 - acc: 0.93 - ETA: 35:25 - loss: 0.2414 - acc: 0.93 - ETA: 35:22 - loss: 0.2417 - acc: 0.93 - ETA: 35:19 - loss: 0.2417 - acc: 0.93 - ETA: 35:15 - loss: 0.2418 - acc: 0.93 - ETA: 35:12 - loss: 0.2414 - acc: 0.93 - ETA: 35:10 - loss: 0.2414 - acc: 0.93 - ETA: 35:07 - loss: 0.2413 - acc: 0.93 - ETA: 35:03 - loss: 0.2412 - acc: 0.93 - ETA: 35:00 - loss: 0.2408 - acc: 0.93 - ETA: 34:57 - loss: 0.2407 - acc: 0.93 - ETA: 34:54 - loss: 0.2406 - acc: 0.93 - ETA: 34:51 - loss: 0.2410 - acc: 0.93 - ETA: 34:48 - loss: 0.2410 - acc: 0.93 - ETA: 34:45 - loss: 0.2412 - acc: 0.93 - ETA: 34:43 - loss: 0.2410 - acc: 0.93 - ETA: 34:39 - loss: 0.2410 - acc: 0.93 - ETA: 34:36 - loss: 0.2407 - acc: 0.93 - ETA: 34:33 - loss: 0.2413 - acc: 0.93 - ETA: 34:31 - loss: 0.2420 - acc: 0.93 - ETA: 34:28 - loss: 0.2427 - acc: 0.93 - ETA: 34:24 - loss: 0.2424 - acc: 0.93 - ETA: 34:21 - loss: 0.2424 - acc: 0.93 - ETA: 34:18 - loss: 0.2421 - acc: 0.93 - ETA: 34:15 - loss: 0.2421 - acc: 0.93 - ETA: 34:12 - loss: 0.2421 - acc: 0.93 - ETA: 34:09 - loss: 0.2421 - acc: 0.93 - ETA: 34:05 - loss: 0.2422 - acc: 0.93 - ETA: 34:02 - loss: 0.2423 - acc: 0.93 - ETA: 33:59 - loss: 0.2422 - acc: 0.93 - ETA: 33:56 - loss: 0.2418 - acc: 0.93 - ETA: 33:53 - loss: 0.2415 - acc: 0.93 - ETA: 33:50 - loss: 0.2418 - acc: 0.93 - ETA: 33:47 - loss: 0.2414 - acc: 0.93 - ETA: 33:44 - loss: 0.2411 - acc: 0.93 - ETA: 33:41 - loss: 0.2410 - acc: 0.93 - ETA: 33:39 - loss: 0.2408 - acc: 0.93 - ETA: 33:36 - loss: 0.2404 - acc: 0.93 - ETA: 33:32 - loss: 0.2405 - acc: 0.93 - ETA: 33:29 - loss: 0.2402 - acc: 0.93 - ETA: 33:26 - loss: 0.2400 - acc: 0.93 - ETA: 33:23 - loss: 0.2398 - acc: 0.93 - ETA: 33:20 - loss: 0.2395 - acc: 0.93 - ETA: 33:17 - loss: 0.2394 - acc: 0.93 - ETA: 33:13 - loss: 0.2399 - acc: 0.93 - ETA: 33:10 - loss: 0.2396 - acc: 0.93 - ETA: 33:07 - loss: 0.2394 - acc: 0.93 - ETA: 33:04 - loss: 0.2402 - acc: 0.93 - ETA: 33:01 - loss: 0.2398 - acc: 0.93 - ETA: 32:58 - loss: 0.2397 - acc: 0.93 - ETA: 32:55 - loss: 0.2396 - acc: 0.93 - ETA: 32:52 - loss: 0.2402 - acc: 0.93 - ETA: 32:49 - loss: 0.2400 - acc: 0.93 - ETA: 32:46 - loss: 0.2398 - acc: 0.93 - ETA: 32:43 - loss: 0.2395 - acc: 0.93 - ETA: 32:39 - loss: 0.2393 - acc: 0.93 - ETA: 32:36 - loss: 0.2391 - acc: 0.93 - ETA: 32:33 - loss: 0.2389 - acc: 0.93 - ETA: 32:30 - loss: 0.2386 - acc: 0.93 - ETA: 32:27 - loss: 0.2383 - acc: 0.93 - ETA: 32:24 - loss: 0.2380 - acc: 0.93 - ETA: 32:21 - loss: 0.2379 - acc: 0.93 - ETA: 32:18 - loss: 0.2375 - acc: 0.93 - ETA: 32:14 - loss: 0.2372 - acc: 0.93 - ETA: 32:11 - loss: 0.2370 - acc: 0.93 - ETA: 32:08 - loss: 0.2369 - acc: 0.93 - ETA: 32:05 - loss: 0.2366 - acc: 0.93 - ETA: 32:03 - loss: 0.2363 - acc: 0.93 - ETA: 32:00 - loss: 0.2364 - acc: 0.93 - ETA: 31:56 - loss: 0.2361 - acc: 0.93 - ETA: 31:53 - loss: 0.2358 - acc: 0.93 - ETA: 31:50 - loss: 0.2375 - acc: 0.93 - ETA: 31:47 - loss: 0.2373 - acc: 0.93 - ETA: 31:44 - loss: 0.2369 - acc: 0.9381"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25984/39200 [==================>...........] - ETA: 31:41 - loss: 0.2366 - acc: 0.93 - ETA: 31:38 - loss: 0.2373 - acc: 0.93 - ETA: 31:35 - loss: 0.2371 - acc: 0.93 - ETA: 31:32 - loss: 0.2369 - acc: 0.93 - ETA: 31:29 - loss: 0.2369 - acc: 0.93 - ETA: 31:26 - loss: 0.2374 - acc: 0.93 - ETA: 31:23 - loss: 0.2371 - acc: 0.93 - ETA: 31:20 - loss: 0.2367 - acc: 0.93 - ETA: 31:17 - loss: 0.2364 - acc: 0.93 - ETA: 31:14 - loss: 0.2361 - acc: 0.93 - ETA: 31:11 - loss: 0.2360 - acc: 0.93 - ETA: 31:08 - loss: 0.2358 - acc: 0.93 - ETA: 31:05 - loss: 0.2361 - acc: 0.93 - ETA: 31:02 - loss: 0.2359 - acc: 0.93 - ETA: 30:59 - loss: 0.2356 - acc: 0.93 - ETA: 30:56 - loss: 0.2354 - acc: 0.93 - ETA: 30:53 - loss: 0.2351 - acc: 0.93 - ETA: 30:49 - loss: 0.2348 - acc: 0.93 - ETA: 30:47 - loss: 0.2346 - acc: 0.93 - ETA: 30:43 - loss: 0.2346 - acc: 0.93 - ETA: 30:40 - loss: 0.2344 - acc: 0.93 - ETA: 30:38 - loss: 0.2357 - acc: 0.93 - ETA: 30:34 - loss: 0.2357 - acc: 0.93 - ETA: 30:31 - loss: 0.2358 - acc: 0.93 - ETA: 30:28 - loss: 0.2356 - acc: 0.93 - ETA: 30:25 - loss: 0.2354 - acc: 0.93 - ETA: 30:22 - loss: 0.2352 - acc: 0.93 - ETA: 30:19 - loss: 0.2349 - acc: 0.93 - ETA: 30:16 - loss: 0.2347 - acc: 0.93 - ETA: 30:13 - loss: 0.2347 - acc: 0.93 - ETA: 30:10 - loss: 0.2347 - acc: 0.93 - ETA: 30:07 - loss: 0.2349 - acc: 0.93 - ETA: 30:04 - loss: 0.2347 - acc: 0.93 - ETA: 30:01 - loss: 0.2346 - acc: 0.93 - ETA: 29:58 - loss: 0.2344 - acc: 0.93 - ETA: 29:55 - loss: 0.2344 - acc: 0.93 - ETA: 29:52 - loss: 0.2342 - acc: 0.93 - ETA: 29:48 - loss: 0.2341 - acc: 0.93 - ETA: 29:45 - loss: 0.2338 - acc: 0.93 - ETA: 29:43 - loss: 0.2343 - acc: 0.93 - ETA: 29:39 - loss: 0.2340 - acc: 0.93 - ETA: 29:36 - loss: 0.2340 - acc: 0.93 - ETA: 29:33 - loss: 0.2345 - acc: 0.93 - ETA: 29:30 - loss: 0.2342 - acc: 0.93 - ETA: 29:27 - loss: 0.2340 - acc: 0.93 - ETA: 29:24 - loss: 0.2344 - acc: 0.93 - ETA: 29:21 - loss: 0.2343 - acc: 0.93 - ETA: 29:18 - loss: 0.2340 - acc: 0.93 - ETA: 29:15 - loss: 0.2338 - acc: 0.93 - ETA: 29:12 - loss: 0.2340 - acc: 0.93 - ETA: 29:09 - loss: 0.2348 - acc: 0.93 - ETA: 29:05 - loss: 0.2345 - acc: 0.93 - ETA: 29:03 - loss: 0.2344 - acc: 0.93 - ETA: 28:59 - loss: 0.2343 - acc: 0.93 - ETA: 28:56 - loss: 0.2344 - acc: 0.93 - ETA: 28:53 - loss: 0.2353 - acc: 0.93 - ETA: 28:50 - loss: 0.2353 - acc: 0.93 - ETA: 28:47 - loss: 0.2351 - acc: 0.93 - ETA: 28:44 - loss: 0.2349 - acc: 0.93 - ETA: 28:41 - loss: 0.2350 - acc: 0.93 - ETA: 28:38 - loss: 0.2356 - acc: 0.93 - ETA: 28:35 - loss: 0.2355 - acc: 0.93 - ETA: 28:32 - loss: 0.2353 - acc: 0.93 - ETA: 28:29 - loss: 0.2352 - acc: 0.93 - ETA: 28:26 - loss: 0.2355 - acc: 0.93 - ETA: 28:23 - loss: 0.2365 - acc: 0.93 - ETA: 28:20 - loss: 0.2363 - acc: 0.93 - ETA: 28:16 - loss: 0.2360 - acc: 0.93 - ETA: 28:13 - loss: 0.2358 - acc: 0.93 - ETA: 28:10 - loss: 0.2357 - acc: 0.93 - ETA: 28:07 - loss: 0.2356 - acc: 0.93 - ETA: 28:04 - loss: 0.2354 - acc: 0.93 - ETA: 28:01 - loss: 0.2351 - acc: 0.93 - ETA: 27:58 - loss: 0.2359 - acc: 0.93 - ETA: 27:54 - loss: 0.2357 - acc: 0.93 - ETA: 27:51 - loss: 0.2358 - acc: 0.93 - ETA: 27:48 - loss: 0.2358 - acc: 0.93 - ETA: 27:45 - loss: 0.2355 - acc: 0.93 - ETA: 27:42 - loss: 0.2366 - acc: 0.93 - ETA: 27:39 - loss: 0.2364 - acc: 0.93 - ETA: 27:36 - loss: 0.2362 - acc: 0.93 - ETA: 27:33 - loss: 0.2359 - acc: 0.93 - ETA: 27:29 - loss: 0.2358 - acc: 0.93 - ETA: 27:26 - loss: 0.2356 - acc: 0.93 - ETA: 27:23 - loss: 0.2356 - acc: 0.93 - ETA: 27:20 - loss: 0.2353 - acc: 0.93 - ETA: 27:17 - loss: 0.2351 - acc: 0.93 - ETA: 27:14 - loss: 0.2349 - acc: 0.93 - ETA: 27:11 - loss: 0.2361 - acc: 0.93 - ETA: 27:07 - loss: 0.2359 - acc: 0.93 - ETA: 27:04 - loss: 0.2363 - acc: 0.93 - ETA: 27:01 - loss: 0.2363 - acc: 0.93 - ETA: 26:58 - loss: 0.2361 - acc: 0.93 - ETA: 26:55 - loss: 0.2360 - acc: 0.93 - ETA: 26:52 - loss: 0.2357 - acc: 0.93 - ETA: 26:49 - loss: 0.2361 - acc: 0.93 - ETA: 26:46 - loss: 0.2359 - acc: 0.93 - ETA: 26:43 - loss: 0.2360 - acc: 0.93 - ETA: 26:40 - loss: 0.2357 - acc: 0.93 - ETA: 26:36 - loss: 0.2356 - acc: 0.93 - ETA: 26:33 - loss: 0.2355 - acc: 0.93 - ETA: 26:30 - loss: 0.2354 - acc: 0.93 - ETA: 26:27 - loss: 0.2365 - acc: 0.93 - ETA: 26:24 - loss: 0.2369 - acc: 0.93 - ETA: 26:21 - loss: 0.2368 - acc: 0.93 - ETA: 26:18 - loss: 0.2365 - acc: 0.93 - ETA: 26:15 - loss: 0.2371 - acc: 0.93 - ETA: 26:11 - loss: 0.2368 - acc: 0.93 - ETA: 26:08 - loss: 0.2366 - acc: 0.93 - ETA: 26:05 - loss: 0.2365 - acc: 0.93 - ETA: 26:02 - loss: 0.2363 - acc: 0.93 - ETA: 25:59 - loss: 0.2362 - acc: 0.93 - ETA: 25:56 - loss: 0.2360 - acc: 0.93 - ETA: 25:53 - loss: 0.2359 - acc: 0.93 - ETA: 25:50 - loss: 0.2356 - acc: 0.93 - ETA: 25:46 - loss: 0.2353 - acc: 0.94 - ETA: 25:43 - loss: 0.2350 - acc: 0.94 - ETA: 25:40 - loss: 0.2349 - acc: 0.94 - ETA: 25:37 - loss: 0.2346 - acc: 0.94 - ETA: 25:34 - loss: 0.2346 - acc: 0.94 - ETA: 25:31 - loss: 0.2344 - acc: 0.94 - ETA: 25:28 - loss: 0.2342 - acc: 0.94 - ETA: 25:25 - loss: 0.2342 - acc: 0.94 - ETA: 25:22 - loss: 0.2343 - acc: 0.94 - ETA: 25:19 - loss: 0.2341 - acc: 0.94 - ETA: 25:16 - loss: 0.2339 - acc: 0.94 - ETA: 25:13 - loss: 0.2337 - acc: 0.94 - ETA: 25:10 - loss: 0.2337 - acc: 0.94 - ETA: 25:07 - loss: 0.2335 - acc: 0.94 - ETA: 25:03 - loss: 0.2334 - acc: 0.94 - ETA: 25:00 - loss: 0.2333 - acc: 0.94 - ETA: 24:57 - loss: 0.2331 - acc: 0.94 - ETA: 24:54 - loss: 0.2330 - acc: 0.94 - ETA: 24:51 - loss: 0.2329 - acc: 0.94 - ETA: 24:48 - loss: 0.2327 - acc: 0.94 - ETA: 24:45 - loss: 0.2336 - acc: 0.94 - ETA: 24:42 - loss: 0.2333 - acc: 0.94 - ETA: 24:39 - loss: 0.2332 - acc: 0.94 - ETA: 24:36 - loss: 0.2331 - acc: 0.94 - ETA: 24:33 - loss: 0.2329 - acc: 0.94 - ETA: 24:29 - loss: 0.2330 - acc: 0.94 - ETA: 24:26 - loss: 0.2331 - acc: 0.94 - ETA: 24:23 - loss: 0.2336 - acc: 0.94 - ETA: 24:20 - loss: 0.2333 - acc: 0.94 - ETA: 24:17 - loss: 0.2330 - acc: 0.94 - ETA: 24:14 - loss: 0.2328 - acc: 0.94 - ETA: 24:11 - loss: 0.2327 - acc: 0.94 - ETA: 24:08 - loss: 0.2325 - acc: 0.94 - ETA: 24:05 - loss: 0.2322 - acc: 0.94 - ETA: 24:01 - loss: 0.2319 - acc: 0.94 - ETA: 23:58 - loss: 0.2318 - acc: 0.94 - ETA: 23:55 - loss: 0.2316 - acc: 0.94 - ETA: 23:52 - loss: 0.2316 - acc: 0.94 - ETA: 23:49 - loss: 0.2316 - acc: 0.94 - ETA: 23:46 - loss: 0.2314 - acc: 0.94 - ETA: 23:43 - loss: 0.2316 - acc: 0.94 - ETA: 23:40 - loss: 0.2316 - acc: 0.94 - ETA: 23:37 - loss: 0.2315 - acc: 0.94 - ETA: 23:34 - loss: 0.2313 - acc: 0.94 - ETA: 23:30 - loss: 0.2317 - acc: 0.94 - ETA: 23:27 - loss: 0.2317 - acc: 0.94 - ETA: 23:24 - loss: 0.2314 - acc: 0.94 - ETA: 23:21 - loss: 0.2316 - acc: 0.94 - ETA: 23:18 - loss: 0.2319 - acc: 0.94 - ETA: 23:15 - loss: 0.2317 - acc: 0.94 - ETA: 23:12 - loss: 0.2316 - acc: 0.94 - ETA: 23:09 - loss: 0.2315 - acc: 0.94 - ETA: 23:06 - loss: 0.2313 - acc: 0.94 - ETA: 23:02 - loss: 0.2310 - acc: 0.94 - ETA: 22:59 - loss: 0.2309 - acc: 0.94 - ETA: 22:56 - loss: 0.2307 - acc: 0.94 - ETA: 22:53 - loss: 0.2307 - acc: 0.94 - ETA: 22:50 - loss: 0.2305 - acc: 0.94 - ETA: 22:47 - loss: 0.2303 - acc: 0.94 - ETA: 22:44 - loss: 0.2300 - acc: 0.94 - ETA: 22:41 - loss: 0.2302 - acc: 0.94 - ETA: 22:38 - loss: 0.2302 - acc: 0.94 - ETA: 22:34 - loss: 0.2300 - acc: 0.94 - ETA: 22:31 - loss: 0.2298 - acc: 0.94 - ETA: 22:28 - loss: 0.2299 - acc: 0.94 - ETA: 22:25 - loss: 0.2297 - acc: 0.94 - ETA: 22:22 - loss: 0.2297 - acc: 0.94 - ETA: 22:19 - loss: 0.2295 - acc: 0.94 - ETA: 22:16 - loss: 0.2293 - acc: 0.94 - ETA: 22:13 - loss: 0.2291 - acc: 0.94 - ETA: 22:10 - loss: 0.2290 - acc: 0.94 - ETA: 22:06 - loss: 0.2290 - acc: 0.94 - ETA: 22:03 - loss: 0.2287 - acc: 0.94 - ETA: 22:00 - loss: 0.2286 - acc: 0.94 - ETA: 21:57 - loss: 0.2284 - acc: 0.94 - ETA: 21:54 - loss: 0.2288 - acc: 0.94 - ETA: 21:51 - loss: 0.2287 - acc: 0.94 - ETA: 21:48 - loss: 0.2285 - acc: 0.94 - ETA: 21:45 - loss: 0.2283 - acc: 0.94 - ETA: 21:42 - loss: 0.2284 - acc: 0.94 - ETA: 21:38 - loss: 0.2283 - acc: 0.94 - ETA: 21:35 - loss: 0.2282 - acc: 0.94 - ETA: 21:32 - loss: 0.2282 - acc: 0.94 - ETA: 21:29 - loss: 0.2280 - acc: 0.94 - ETA: 21:26 - loss: 0.2280 - acc: 0.94 - ETA: 21:23 - loss: 0.2277 - acc: 0.94 - ETA: 21:20 - loss: 0.2283 - acc: 0.94 - ETA: 21:17 - loss: 0.2283 - acc: 0.94 - ETA: 21:14 - loss: 0.2283 - acc: 0.9412"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28448/39200 [====================>.........] - ETA: 21:10 - loss: 0.2281 - acc: 0.94 - ETA: 21:07 - loss: 0.2279 - acc: 0.94 - ETA: 21:04 - loss: 0.2277 - acc: 0.94 - ETA: 21:01 - loss: 0.2276 - acc: 0.94 - ETA: 20:58 - loss: 0.2276 - acc: 0.94 - ETA: 20:55 - loss: 0.2277 - acc: 0.94 - ETA: 20:52 - loss: 0.2277 - acc: 0.94 - ETA: 20:49 - loss: 0.2274 - acc: 0.94 - ETA: 20:46 - loss: 0.2272 - acc: 0.94 - ETA: 20:44 - loss: 0.2271 - acc: 0.94 - ETA: 20:40 - loss: 0.2270 - acc: 0.94 - ETA: 20:37 - loss: 0.2268 - acc: 0.94 - ETA: 20:34 - loss: 0.2266 - acc: 0.94 - ETA: 20:31 - loss: 0.2271 - acc: 0.94 - ETA: 20:28 - loss: 0.2271 - acc: 0.94 - ETA: 20:25 - loss: 0.2268 - acc: 0.94 - ETA: 20:22 - loss: 0.2281 - acc: 0.94 - ETA: 20:19 - loss: 0.2281 - acc: 0.94 - ETA: 20:16 - loss: 0.2279 - acc: 0.94 - ETA: 20:13 - loss: 0.2276 - acc: 0.94 - ETA: 20:10 - loss: 0.2278 - acc: 0.94 - ETA: 20:06 - loss: 0.2280 - acc: 0.94 - ETA: 20:03 - loss: 0.2280 - acc: 0.94 - ETA: 20:01 - loss: 0.2278 - acc: 0.94 - ETA: 19:57 - loss: 0.2289 - acc: 0.94 - ETA: 19:54 - loss: 0.2287 - acc: 0.94 - ETA: 19:51 - loss: 0.2284 - acc: 0.94 - ETA: 19:48 - loss: 0.2285 - acc: 0.94 - ETA: 19:45 - loss: 0.2285 - acc: 0.94 - ETA: 19:42 - loss: 0.2284 - acc: 0.94 - ETA: 19:39 - loss: 0.2282 - acc: 0.94 - ETA: 19:36 - loss: 0.2280 - acc: 0.94 - ETA: 19:33 - loss: 0.2281 - acc: 0.94 - ETA: 19:30 - loss: 0.2285 - acc: 0.94 - ETA: 19:27 - loss: 0.2285 - acc: 0.94 - ETA: 19:24 - loss: 0.2297 - acc: 0.94 - ETA: 19:21 - loss: 0.2295 - acc: 0.94 - ETA: 19:17 - loss: 0.2295 - acc: 0.94 - ETA: 19:14 - loss: 0.2294 - acc: 0.94 - ETA: 19:11 - loss: 0.2293 - acc: 0.94 - ETA: 19:08 - loss: 0.2291 - acc: 0.94 - ETA: 19:05 - loss: 0.2295 - acc: 0.94 - ETA: 19:02 - loss: 0.2293 - acc: 0.94 - ETA: 18:59 - loss: 0.2291 - acc: 0.94 - ETA: 18:57 - loss: 0.2298 - acc: 0.94 - ETA: 18:54 - loss: 0.2296 - acc: 0.94 - ETA: 18:51 - loss: 0.2304 - acc: 0.94 - ETA: 18:48 - loss: 0.2305 - acc: 0.94 - ETA: 18:45 - loss: 0.2304 - acc: 0.94 - ETA: 18:42 - loss: 0.2304 - acc: 0.94 - ETA: 18:39 - loss: 0.2302 - acc: 0.94 - ETA: 18:36 - loss: 0.2302 - acc: 0.94 - ETA: 18:33 - loss: 0.2310 - acc: 0.94 - ETA: 18:30 - loss: 0.2308 - acc: 0.94 - ETA: 18:27 - loss: 0.2305 - acc: 0.94 - ETA: 18:24 - loss: 0.2303 - acc: 0.94 - ETA: 18:22 - loss: 0.2301 - acc: 0.94 - ETA: 18:19 - loss: 0.2300 - acc: 0.94 - ETA: 18:16 - loss: 0.2301 - acc: 0.94 - ETA: 18:13 - loss: 0.2300 - acc: 0.94 - ETA: 18:10 - loss: 0.2297 - acc: 0.94 - ETA: 18:07 - loss: 0.2296 - acc: 0.94 - ETA: 18:04 - loss: 0.2294 - acc: 0.94 - ETA: 18:01 - loss: 0.2292 - acc: 0.94 - ETA: 17:58 - loss: 0.2292 - acc: 0.94 - ETA: 17:55 - loss: 0.2292 - acc: 0.94 - ETA: 17:52 - loss: 0.2290 - acc: 0.94 - ETA: 17:49 - loss: 0.2290 - acc: 0.94 - ETA: 17:46 - loss: 0.2288 - acc: 0.94 - ETA: 17:43 - loss: 0.2287 - acc: 0.94 - ETA: 17:40 - loss: 0.2288 - acc: 0.94 - ETA: 17:37 - loss: 0.2287 - acc: 0.94 - ETA: 17:34 - loss: 0.2288 - acc: 0.94 - ETA: 17:31 - loss: 0.2286 - acc: 0.94 - ETA: 17:28 - loss: 0.2284 - acc: 0.94 - ETA: 17:25 - loss: 0.2282 - acc: 0.94 - ETA: 17:22 - loss: 0.2280 - acc: 0.9419"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 50, batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this model (trained on only two epochs) performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9800/9800 [==============================] - ETA: 6: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 59 - ETA: 58 - ETA: 58 - ETA: 57 - ETA: 57 - ETA: 56 - ETA: 56 - ETA: 55 - ETA: 55 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 143s 15ms/step\n",
      "Loss = 0.21399107929243116\n",
      "Test Accuracy = 0.931734693877551\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21399107929243116, 0.931734693877551]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:07<00:00, 2775.31it/s]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test_image = []\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    img = image.load_img('test/'+test['filename'][i], target_size=(28,28,1), grayscale=True)\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    test_image.append(img)\n",
    "X_test = np.array(test_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict_y = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({ \"filename\": test['filename'],      \n",
    "                            \"label\": np.argmax(predict_y, axis=1) })\n",
    "    \n",
    "\n",
    "submission.to_csv('Digit_Submission1.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "OEpi5",
   "launcher_item_id": "jK9EQ"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
